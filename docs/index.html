<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>LLM Chronicles</title>
    <link href="https://maxcdn.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Condensed:wght@400;700&display=swap" rel="stylesheet">
    <link href="style.css" rel="stylesheet">

    <meta name="description"
        content="A fast-paced whiteboard animation series unraveling Deep Learning and Large Language Models. Dive into core concepts, from Neural Networks basics to the mechanics of modern models like ChatGPT. Simplified insights, no fluff.">

    <!-- Facebook Meta Tags -->
    <meta property="og:url" content="https://llm-chronicles.com/">
    <meta property="og:type" content="website">
    <meta property="og:title" content="LLM Chronicles">
    <meta property="og:description"
        content="A fast-paced whiteboard animation series unraveling Deep Learning and Large Language Models. Dive into core concepts, from Neural Networks basics to the mechanics of modern models like ChatGPT. Simplified insights, no fluff.">
    <meta property="og:image" content="https://llm-chronicles.com/imgs/preview.png">

    <!-- Twitter Meta Tags -->
    <meta name="twitter:card" content="summary_large_image">
    <meta property="twitter:domain" content="llm-chronicles.com">
    <meta property="twitter:url" content="https://llm-chronicles.com/">
    <meta name="twitter:title" content="LLM Chronicles">
    <meta name="twitter:description"
        content="A fast-paced whiteboard animation series unraveling Deep Learning and Large Language Models. Dive into core concepts, from Neural Networks basics to the mechanics of modern models like ChatGPT. Simplified insights, no fluff.">
    <meta name="twitter:image" content="https://llm-chronicles.com/imgs/preview.png">

</head>

<body>

    <div class="content-wrapper">
        <!-- Hero Banner -->
        <section class="hero text-white text-center d-flex justify-content-center align-items-center">
            <div>
                <img src="imgs/logo.png" alt="LLM Chronicles Logo">
            </div>
        </section>
        <section class="hero2 text-white d-flex justify-content-center align-items-center hidden-xs">
            <div class="content-wrapper d-flex flex-column align-items-center">
                <div>
                    <p>Welcome to the "LLM Chronicles", a fast-paced series of whiteboard animations dedicated to Deep
                        Learning and Large Language Models.
                        Throughout this series, I focus on the big ideas and aim to provide an intuitive but detailed
                        grasp
                        of key
                        concepts, from the basics of Neural Networks to how Recurrent
                        Neural Networks and Transformers function in modern Large Language Models such as those that
                        power
                        ChatGPT. My goal is to help you understand how things work without
                        the fluff.

                    </p>
                    <div class="social-media-links text-center">
                        <!-- YouTube Icon Link -->
                        <a href="https://www.youtube.com/@donatocapitella" target="_blank" class="text-white mr-2">
                            <i class="fab fa-youtube"></i> YouTube
                        </a>
                        <!-- LinkedIn Icon Link -->
                        <a href="https://www.linkedin.com/in/dcapitella" target="_blank" class="text-white">
                            <i class="fab fa-linkedin-in"></i> Linkedin
                        </a>
                    </div>
                </div>


            </div>
        </section>

        <div class="scroll-nav active">
            <button class="base scroll-nav-toggle">
                <i class="fas fa-bars" id="expand-icon" style="display: none;"></i>
                <i class="fas fa-arrow-left" id="collapse-icon"></i>
            </button>
            <div class="scroll-nav-inner">
                <div class="scroll-nav-title">LLM Chronicles</div>
                <div class="scroll-nav-items">
                    <ul class="scroll-nav-list">
                        <!-- Group: Introduction -->
                        <li class="complete">
                            <button class="base scroll-nav-button" data-target="part1"><span>Part 1:
                                    Introduction</span></button>
                            <ul class="scroll-nav-sub-list">
                                <li class="complete">
                                    <a href="#1" class="base scroll-nav-button" data-target="1"><span>#1:
                                            Introduction</span></a>
                                </li>
                            </ul>
                        </li>

                        <!-- Group: Neural Networks Basics -->
                        <li class="complete">
                            <button class="base scroll-nav-button" data-target="part2"><span>Part 2: Neural Networks
                                    Basics</span></button>
                            <ul class="scroll-nav-sub-list">
                                <li class="complete">
                                    <a href="#2.1" class="base scroll-nav-button"><span>#2.1: Neural
                                            Networks and Multi-Layer Perceptrons</span></a>
                                </li>
                                <li class="complete">
                                    <a href="#2.2" class="base scroll-nav-button"><span>#2.2: Multi-Layer Perceptrons
                                            and MNIST Digit Classification using PyTorch (Lab)</span></a>
                                </li>
                                <!-- ... other lessons in this group ... -->
                            </ul>
                        </li>

                        <!-- Group: Gradient Descent -->
                        <li>
                            <button class="base scroll-nav-button" data-target="part3"><span>Part 3: Training Neural
                                    Networks</span></button>
                            <ul class="scroll-nav-sub-list">
                                <li class="complete">
                                    <a href="#3.1" class="base scroll-nav-button"><span>#3.1: Loss
                                            Function and Gradient Descent</span></a>
                                </li>
                                <li>
                                    <a href="#3.2" class="base scroll-nav-button"><span>#3.2:
                                            Gradient Descent in PyTorch with Autograd (Lab)</span></a>
                                </li>
                                <li>
                                    <a href="#3.3" class="base scroll-nav-button"><span>
                                            #3.3: Training with Gradient Descent and Optimizations</span></a>
                                </li>
                                <li>
                                    <a href="#3.4" class="base scroll-nav-button"><span>
                                            #3.4: Training the MNIST Perceptron with PyTorch (Lab)</span></a>
                                </li>
                                <li>
                                    <a href="#3.5" class="base scroll-nav-button"><span>
                                            #3.5: Evaluation, Overfitting and Underfitting</span></a>
                                </li>
                            </ul>
                        </li>

                        <li>
                            <button class="base scroll-nav-button" data-target="part4"><span>Part 4: Language models
                                    with
                                    RNNs</span></button>
                            <ul class="scroll-nav-sub-list">
                                <li class="complete">
                                    <a href="#4.1" class="base scroll-nav-button"><span>#4.1: Recurrent Neural
                                            Networks</span></a>
                                </li>
                                <li>
                                    <a href="#4.2" class="base scroll-nav-button"><span>#4.2: RNN and LSTM Cells in
                                            PyTorch (Lab)</span></a>
                                </li>
                                <li>
                                    <a href="#4.3" class="base scroll-nav-button"><span>#4.3: Language Modeling with
                                            RNNs</span></a>
                                </li>
                                <li>
                                    <a href="#4.4" class="base scroll-nav-button"><span>#4.4: Building a Word-Level
                                            Language Model in PyTorch(Lab)</span></a>
                                </li>
                                <li>
                                    <a href="#4.5" class="base scroll-nav-button"><span>#4.5: Encoder/Decoder RNN for
                                            Language Translation (Seq2Seq)</span></a>
                                </li>
                                <li>
                                    <a href="#4.6" class="base scroll-nav-button"><span>#4.6: Building an
                                            Encoder/Decoder RNN in PyTorch (Lab)</span></a>
                                </li>
                                <li>
                                    <a href="#4.7" class="base scroll-nav-button"><span>#4.7: Attention Mechanism for
                                            Seq2Seq RNNs</span></a>
                                </li>
                                <li>
                                    <a href="#4.8" class="base scroll-nav-button"><span>#4.8: Adding Attention to the
                                            Language Translation RNN in PyTorch (Lab)</span></a>
                                </li>
                            </ul>
                        </li>

                        <li>
                            <button class="base scroll-nav-button" data-target="part5"><span>Part 5: Large Language
                                    Models (LLMs)</span></button>
                            <ul class="scroll-nav-sub-list">
                                <li class="complete">
                                    <a href="#5.1" class="base scroll-nav-button"><span>#5.1: Transformers and
                                            Self-Attention</span></a>
                                </li>
                                <li>
                                    <a href="#5.2" class="base scroll-nav-button"><span>#5.2: Making LLMs from
                                            Transformers (BERT, Encoder-based)</span></a>
                                </li>
                                <li>
                                    <a href="#5.3" class="base scroll-nav-button"><span>#5.3: Fine-tuning
                                            DistilBERT for Sentiment Analysis (Lab)
                                        </span></a>
                                </li>
                                <li>
                                    <a href="#5.4" class="base scroll-nav-button"><span>#5.4: GPT, Instruction
                                            Fine-Tuning, RLHF
                                        </span></a>
                                </li>
                                <li>
                                    <a href="#5.5" class="base scroll-nav-button"><span>#5.5: Running Gemma 2B and
                                            Llama-2 7B with Quantization
                                        </span></a>
                                </li>
                                <li>
                                    <a href="#5.6" class="base scroll-nav-button"><span>#5.6: LLM Limitations and
                                            Challenges</span></a>
                                </li>

                            </ul>
                        </li>
                        <li>
                            <button class="base scroll-nav-button" data-target="part6"><span>Part 6: LLM
                                    Applications and Advancements</span></button>
                            <ul class="scroll-nav-sub-list">
                                <li>
                                    <a href="#6.1" class="base scroll-nav-button"><span>#6.1: Retrieval Augmented
                                            Generation - RAG (Part 1)</span></a>
                                </li>
                                <li>
                                    <a href="#6.2" class="base scroll-nav-button"><span>#6.2: Retrieval Augmented
                                            Generation - RAG (Part 2)</span></a>
                                </li>
                                <li>
                                    <a href="#6.3" class="base scroll-nav-button"><span>#6.3: Multi-Modal LLMs for
                                            Image, Sound and Video</span></a>
                                </li>
                                <li>
                                    <a href="#6.4" class="base scroll-nav-button"><span>#6.4: LLM Agents with ReAct
                                            (Reason + Act)</span></a>
                                </li>
                                <li>
                                    <a href="#6.5" class="base scroll-nav-button"><span>#6.5: Quantization and
                                            Distillation</span></a>
                                </li>
                                <li>
                                    <a href="#6.6" class="base scroll-nav-button"><span>#6.6: Transformer Architecture
                                            Improvements (MoE, Flash Attention)</span></a>
                                </li>

                            </ul>
                        </li>

                        <!-- ... other groups ... -->
                    </ul>
                </div>
            </div>
        </div>



        <!-- YouTube Videos -->
        <section class="container-md mt-5">
            <!-- #1: Introduction -->
            <h2 id="part1">Part 1: Introduction</h2>

            <div class="video-section">
                <div id="1" class="row mb-4 video">
                    <div class="col-md-4 thumbnail-wrapper">
                        <a target="_blank" href="https://youtu.be/rRdUbdhHjy0">
                            <img src="imgs/thumb1.png" alt="Video Thumbnail" class="img-fluid">
                        </a>
                    </div>
                    <div class="col-md-8 d-flex flex-column">
                        <h3>#1: Introduction and Roadmap</h3>
                        <div class="video-description">
                            <p>Welcome to the "LLM Chronicles". In this introductory video, I'll lay out the roadmap for
                                navigating the world of Large Language Models. Throughout this series, I'll focus on the
                                big
                                ideas and aim to provide an intuitive grasp of key concepts, from the basics of neural
                                networks to how Recurrent Neural Networks and Transformers function. My goal is to help
                                you
                                understand how things work without the fluff.</p>
                            <p>Join me as I set the course for a clear and intuitive exploration of LLMs.</p>
                        </div>
                        <a target="_blank" href="#" class="show-more" style="display: none;">more...</a>
                        <div class="reference-links mt-auto">
                            <a href="#" data-toggle="modal" data-target="#pdfModal"
                                data-pdf-url="pdfs/llm-chronicles-1_mindmap_roadmap.pdf"><i class="fas fa-file-pdf"></i>
                                Roadmap</a>

                        </div>
                    </div>
                </div>
            </div>

            <h2 id="part2">Part 2: Neural Networks Basics</h2>
            <div class="video-section">
                <!-- #2.1: Neural Networks and Multi-Layer Perceptrons -->
                <div id="2.1" class="row mb-4 video">
                    <div class="col-md-4 thumbnail-wrapper">
                        <a target="_blank" href="https://youtu.be/esyf8hK65kc">
                            <img src="imgs/thumb2_1.png" alt="Video Thumbnail" class="img-fluid">
                        </a>
                    </div>
                    <div class="col-md-8 d-flex flex-column">
                        <h3>#2.1: Neural Networks and Multi-Layer Perceptrons</h3>
                        <div class="video-description">
                            <p>In this episode of the LLM Chronicles, we'll cover the basics of artificial neurons, see
                                how
                                they form multi-layer perceptrons, and learn how to model inputs and outputs for
                                effective
                                network processing. We'll also touch on vectorialization and its role in optimizing
                                neural
                                network implementations on GPUs.</p>
                        </div>
                        <a target="_blank" href="#" class="show-more" style="display: none;">more...</a>
                        <div class="reference-links mt-auto">
                            <a href="#" data-toggle="modal" data-target="#pdfModal"
                                data-pdf-url="pdfs/llm-chronicles-2.1_mindmap_multi-layer-perceprtons.pdf"><i
                                    class="fas fa-file-pdf"></i>
                                Multi-Player Perceptrons - Mindmap</a>
                        </div>
                    </div>
                </div>

                <!-- #2.2: Multi-Layer Perceptrons and MNIST Digit Classification using PyTorch (Lab) -->
                <div id="2.2" class="row mb-4 video">
                    <div class="col-md-4 thumbnail-wrapper">
                        <a target="_blank" href="https://youtu.be/jpIAx_GZ_4U">
                            <img src="imgs/thumb2_2.png" alt="Video Thumbnail" class="img-fluid">
                        </a>
                    </div>
                    <div class="col-md-8 d-flex flex-column">
                        <h3>#2.2: Multi-Layer Perceptrons and MNIST Digit Classification using PyTorch
                            (Lab)
                        </h3>
                        <div class="video-description">
                            <p>Welcome to the first hands-on lab session of the LLM Chronicles. Here we first implement
                                Multi-Layer Perceptrons (MLPs) from scratch using matrix operations with NumPy.
                                Following
                                that, we set up an MLP using PyTorch for digit classification on the MNIST dataset.</p>
                        </div>
                        <a target="_blank" href="#" class="show-more" style="display: none;">more...</a>
                        <div class="reference-links mt-auto">
                            <a
                                href="https://colab.research.google.com/drive/1HzVYVikFmp2S_ks0nbkDcuwoTZBdyUbr?usp=sharing"><i
                                    class="fas fa-code"></i> Lab Notebook</a>
                        </div>
                    </div>
                </div>
            </div>
            <!-- Section 3 -->
            <h2 id="part3">Part 3: Training Neural Networks</h2>
            <div class="video-section">
                <!-- #3.1: Loss Function and Gradient Descent -->
                <div id="3.1" class="row mb-4 video">
                    <div class="col-md-4 thumbnail-wrapper">
                        <a target="_blank" href="https://youtu.be/I-yhEYZTXlY">
                            <img src="imgs/thumb3_1.png" alt="Video Thumbnail" class="img-fluid">
                        </a>
                    </div>
                    <div class="col-md-8 d-flex flex-column">
                        <h3>#3.1: Loss Function and Gradient Descent</h3>
                        <div class="video-description">
                            <p>In this episode of the LLM Chronicles, we'll cover how to prepare datasets for effective
                                training of neural networks. We'll then look at how the problem of training a network
                                can be
                                defined in terms of minimizing the loss function and we'll close by looking at how we
                                can
                                achieve this using gradient descent.</p>
                        </div>
                        <a target="_blank" href="#" class="show-more" style="display: none;">more...</a>
                        <div class="reference-links mt-auto">
                            <a href="#" data-toggle="modal" data-target="#pdfModal"
                                data-pdf-url="pdfs/llm-chronicles-3.1_mindmap_preparing-dataset.pdf"><i
                                    class="fas fa-file-pdf"></i>
                                Preparing Datasets - Mindmap</a>
                            <a href="#" data-toggle="modal" data-target="#pdfModal"
                                data-pdf-url="pdfs/llm-chronicles-3.1_mindmap_loss-function-and-gradient-descent.pdf"><i
                                    class="fas fa-file-pdf"></i>
                                Loss and Gradient Descent - Mindmap</a>
                        </div>
                    </div>
                </div>

                <!-- #3.2: Gradient Descent in PyTorch with Autograd (Lab) -->
                <div id="3.2" class="row mb-4 video">
                    <div class="col-md-4 thumbnail-wrapper">
                        <a target="_blank" href="https://youtu.be/ijhk5n2hJOY">
                            <img src="imgs/thumb3_2.png" alt="Video Thumbnail" class="img-fluid">
                        </a>
                    </div>
                    <div class="col-md-8 d-flex flex-column">
                        <h3>#3.2: Gradient Descent in PyTorch with Autograd (Lab)</h3>
                        <div class="video-description">
                            <p>In this hands-on lab we look at how PyTorch uses the Autograd library to implement the
                                concepts of derivatives and gradient descent that we saw in the previous episode.</p>
                        </div>
                        <a target="_blank" href="#" class="show-more" style="display: none;">more...</a>
                        <div class="reference-links mt-auto">
                            <a
                                href="https://colab.research.google.com/drive/1GVTet0rr_3Q5zGmfeQE7dA0bPsuGkVWd?usp=sharing"><i
                                    class="fas fa-code"></i> Lab Notebook</a>
                        </div>
                    </div>
                </div>

                <div id="3.3" class="row mb-4 video">
                    <div class="col-md-4 thumbnail-wrapper">
                        <a target="_blank" href="https://youtu.be/TdY-DD_OYwQ">
                            <img src="imgs/thumb3_3.png" alt="Video Thumbnail" class="img-fluid">
                        </a>
                    </div>
                    <div class="col-md-8 d-flex flex-column">
                        <h3>#3.3: Training with Gradient Descent and Optimizations(mini-batch, momentum, RMSProp, ADAM)
                        </h3>
                        <div class="video-description">
                            <p>This is an exciting episode of the LLM Chronicles as we finally cover how neural networks
                                are trained in practice and what a training loop looks like. We also cover a training
                                method called mini-batch updates, comparing it with full-batch and incremental updates.
                                We end this episode by looking at common optimizations of gradient descent, such as
                                momentum, RMSProp and ADAM.</p>
                        </div>
                        <a target="_blank" href="#" class="show-more" style="display: none;">more...</a>
                        <div class="reference-links mt-auto">
                            <a href="#" data-toggle="modal" data-target="#pdfModal"
                                data-pdf-url="pdfs/llm-chronicles-3.3_training-loop-and-optimizations-mini-batch-sgd-adam.pdf"><i
                                    class="fas fa-file-pdf"></i> Training with Gradient Descent and
                                Optimizations(mini-batch, momentum, RMSProp, ADAM)</a>
                        </div>
                    </div>
                </div>

                <div id="3.4" class="row mb-4 video">
                    <div class="col-md-4 thumbnail-wrapper">
                        <a target="_blank" href="https://youtu.be/AaWHOe4r90k">
                            <img src="imgs/thumb3_4.png" alt="Video Thumbnail" class="img-fluid">
                        </a>
                    </div>
                    <div class="col-md-8 d-flex flex-column">
                        <h3>#3.4: Training the MNIST Perceptron with PyTorch (Lab)</h3>
                        <div class="video-description">
                            <p>In this lab we'll finally put everything together and using PyTorch we'll train our model
                                to recognize the
                                MNIST hand-written digits, using gradient descent and mini-batch updates.</p>
                        </div>
                        <a target="_blank" href="#" class="show-more" style="display: none;">more...</a>
                        <div class="reference-links mt-auto">
                            <a target="_blank"
                                href="https://colab.research.google.com/drive/1IXtVJ9iO2RuXQlRwVr9Tj0w407oynBFF?usp=sharing"><i
                                    class="fas fa-code"></i> Lab Notebook</a>
                        </div>
                    </div>
                </div>

                <div id="3.5" class="row mb-4 video">
                    <div class="col-md-4 thumbnail-wrapper">
                        <a target="_blank" href="https://youtu.be/uz8GTmit1p4">
                            <img src="imgs/thumb3_5.png" alt="Video Thumbnail" class="img-fluid">
                        </a>
                    </div>
                    <div class="col-md-8 d-flex flex-column">
                        <h3>#3.5: Evaluation, Overfitting and Underfitting + Bonus Lab</h3>
                        <div class="video-description">
                            <p>In this episode we look at common metrics to evaluate our models, such as accuracy,
                                precision and recall. We also look at overfitting and underfitting, two common issues
                                that can arise when training models, and what we can do to mitigate them.</p>
                        </div>
                        <a target="_blank" href="#" class="show-more" style="display: none;">more...</a>
                        <div class="reference-links mt-auto">
                            <a href="#" data-toggle="modal" data-target="#pdfModal"
                                data-pdf-url="pdfs/llm-chronicles-3.5_evaluation_metrics.pdf"><i
                                    class="fas fa-file-pdf"></i> Evaluation, Overfitting and Underfitting</a>
                            <a target="_blank"
                                href="https://colab.research.google.com/drive/1IXtVJ9iO2RuXQlRwVr9Tj0w407oynBFF?usp=sharing"><i
                                    class="fas fa-code"></i> Lab Notebook</a>
                        </div>
                    </div>
                </div>
            </div>

            <h2 id="part4">Part 4: Language Models with Recurrent Neural Networks</h2>
            <div class="video-section">
                <div id="4.1" class="row mb-4 video">
                    <div class="col-md-4 thumbnail-wrapper">
                        <a target="_blank" href="https://youtu.be/qQvHC027k9M">
                            <img src="imgs/thumb4_1.png" alt="Video Thumbnail" class="img-fluid">
                        </a>
                    </div>
                    <div class="col-md-8 d-flex flex-column">
                        <h3>#4.1: Recurrent Neural Networks</h3>
                        <div class="video-description">
                            <p>In this episode we'll look at how to model sequences of data, such as natural language,
                                using Recurrent Neural Networks. We'll peak into the implementation of an RNN layer,
                                looking at all the operations involved in the forward pass. Finally, we'll look at how
                                good they are at remembering information from the past and at common issues that arise
                                during training (vanishing and exploding gradients). We'll finish by looking at how Long
                                short-term memory cells can help mitigate some of these issues.</p>
                        </div>
                        <a target="_blank" href="#" class="show-more" style="display: none;">more...</a>
                        <div class="reference-links mt-auto">
                            <a href="#" data-toggle="modal" data-target="#pdfModal"
                                data-pdf-url="pdfs/llm-chronicles-4.1_recurrent_neural_networks.pdf"><i
                                    class="fas fa-file-pdf"></i> Recurrent Neural Networks</a>
                        </div>
                    </div>
                </div>

                <div id="4.2" class="row mb-4 video">
                    <div class="col-md-4 thumbnail-wrapper">
                        <a target="_blank" href="https://youtu.be/yvab5-qKHUw">
                            <img src="imgs/thumb4_2.png" alt="Video Thumbnail" class="img-fluid">
                        </a>
                    </div>
                    <div class="col-md-8 d-flex flex-column">
                        <h3>#4.2: RNN and LSTM Cells in PyTorch (Lab)</h3>
                        <div class="video-description">
                            <p>In this short lab we'll first implement an RNN layer by hand to understand all the
                                computations in the forward pass. after that, we'll look at PyTorch's implementation of
                                RNNs and LSTM cells.
                            </p>
                        </div>
                        <a target="_blank" href="#" class="show-more" style="display: none;">more...</a>
                        <div class="reference-links mt-auto">
                            <a
                                href="https://colab.research.google.com/drive/114x0tjhDtAx-ZCsi-wv2XhhVfIc2x0zg?usp=sharing"><i
                                    class="fas fa-code"></i> Lab Notebook</a>
                        </div>
                    </div>
                </div>

                <div id="4.3" class="row mb-4 video">
                    <div class="col-md-4 thumbnail-wrapper">
                        <a target="_blank" href="https://youtu.be/yV8S6di-ATs">
                            <img src="imgs/thumb4_3.png" alt="Video Thumbnail" class="img-fluid">
                        </a>
                    </div>
                    <div class="col-md-8 d-flex flex-column">
                        <h3>#4.3: Language Modeling with RNNs</h3>
                        <div class="video-description">
                            <p>In this episode we'll look at how to model sequences of data, such as natural language,
                                using Recurrent Neural Networks. We'll peak into the implementation of an RNN layer,
                                looking at all the operations involved in the forward pass. Finally, we'll look at how
                                good they are at remembering information from the past and at common issues that arise
                                during training (vanishing and exploding gradients). We'll finish by looking at how Long
                                short-term memory cells can help mitigate some of these issues.</p>
                        </div>
                        <a target="_blank" href="#" class="show-more" style="display: none;">more...</a>
                        <div class="reference-links mt-auto">
                            <a href="#" data-toggle="modal" data-target="#pdfModal"
                                data-pdf-url="pdfs/llm-chronicles-4.3_language_modeling_embeddings_temperature.pdf"><i
                                    class="fas fa-file-pdf"></i> Language Modeling, Embeddings, Temperature</a>
                        </div>
                    </div>
                </div>

                <div id="4.4" class="row mb-4 video">
                    <div class="col-md-4 thumbnail-wrapper">
                        <a target="_blank" href="https://youtu.be/nzRIXaYAaqE">
                            <img src="imgs/thumb4_4.png" alt="Video Thumbnail" class="img-fluid">
                        </a>
                    </div>
                    <div class="col-md-8 d-flex flex-column">
                        <h3>#4.4: Building a Word-Level Language Model in PyTorch(Lab)</h3>
                        <div class="video-description">
                            <p>In this lab we build a word-level language model using an RNN in PyTorch.
                            </p>
                        </div>
                        <a target="_blank" href="#" class="show-more" style="display: none;">more...</a>
                        <div class="reference-links mt-auto">
                            <a
                                href="https://colab.research.google.com/drive/1-Nlo4B36oTB0ErIJropiPfrlQWk8qK6f?usp=sharing"><i
                                    class="fas fa-code"></i> Lab Notebook</a>
                        </div>
                    </div>
                </div>

                <div id="4.5" class="row mb-4 video">
                    <div class="col-md-4 thumbnail-wrapper">
                        <a target="_blank" href="https://youtu.be/YkvT_P6Ztbc">
                            <img src="imgs/thumb4_5.png" alt="Video Thumbnail" class="img-fluid">
                        </a>
                    </div>
                    <div class="col-md-8 d-flex flex-column">
                        <h3>#4.5: Encoder/Decoder RNN for Language Translation (Seq2Seq)</h3>
                        <div class="video-description">
                            <p>In this episode we explore the seminal work on Language Translation with Recurrent Neural
                                Networks(RNNs) that sparked key innovations that power advanced language models like
                                OpenAI’s ChatGPT and Google’s Gemini. Specifically, we'll look at how the
                                encoder/decoder architecture can be used to solve the problem of alignment for seq2seq
                                tasks such as language translation, as described by Ilya Sutskever at al. in a 2014
                                paper titled “Sequence to Sequence Learning with Neural Networks”.
                            </p>
                        </div>
                        <a target="_blank" href="#" class="show-more" style="display: none;">more...</a>
                        <div class="reference-links mt-auto">
                            <a href="#" data-toggle="modal" data-target="#pdfModal"
                                data-pdf-url="pdfs/llm-chronicles-4.5_encoder_decoder_rrn_for_translation.pdf"><i
                                    class="fas fa-file-pdf"></i> Encoder/Decoder RNN for Language Translation</a>
                        </div>
                    </div>
                </div>

                <div id="4.6" class="row mb-4 video">
                    <div class="col-md-4 thumbnail-wrapper">
                        <a target="_blank" href="https://www.youtube.com/watch?v=QQEL7MC0u1E">
                            <img src="imgs/thumb4_6.png" alt="Video Thumbnail" class="img-fluid">
                        </a>
                    </div>
                    <div class="col-md-8 d-flex flex-column">
                        <h3>#4.6: Building an Encoder/Decoder RNN in PyTorch (Lab)</h3>
                        <div class="video-description">
                            <p>In this lab we'll use PyTorch to build a encoder/decoder RNN for language translation,
                                similar to the model described in the 2014 paper "Sequence to Sequence Learning with
                                Neural Networks” by OpenAI's chief scientist Ilya Sutskever. Our RNN layers will use
                                LSTM (Long Short-Term Memory) cells.
                            </p>
                        </div>
                        <a target="_blank" href="#" class="show-more" style="display: none;">more...</a>
                        <div class="reference-links mt-auto">
                            <a
                                href="https://colab.research.google.com/drive/1GBC7eLlEM-HqKLUuMcFIQdVuYXzLoS_P?usp=sharing"><i
                                    class="fas fa-code"></i> Lab Notebook</a>
                        </div>
                    </div>
                </div>

                <div id="4.7" class="row mb-4 video">
                    <div class="col-md-4 thumbnail-wrapper">
                        <a target="_blank" href="https://youtu.be/PaP_DKuZTBc">
                            <img src="imgs/thumb4_7.png" alt="Video Thumbnail" class="img-fluid">
                        </a>
                    </div>
                    <div class="col-md-8 d-flex flex-column">
                        <h3>#4.7: Attention Mechanism for Seq2Seq RNNs</h3>
                        <div class="video-description">
                            <p>In this episode we’ll cover the so-called "attention mechanism" for seq2seq tasks with
                                Neural Networks. This was first introduced by Bahdanau at al. in 2014 to improve the
                                performance of encoder/decoder RNNs used for language translation and proved fundamental
                                to create the Transformer architecture that powers modern LLMs.</p>
                        </div>
                        <a target="_blank" href="#" class="show-more" style="display: none;">more...</a>
                        <div class="reference-links mt-auto">
                            <a href="#" data-toggle="modal" data-target="#pdfModal"
                                data-pdf-url="pdfs/llm-chronicles-4.7_attention_for_neural_networks.pdf"><i
                                    class="fas fa-file-pdf"></i> Attention for Neural Networks</a>
                        </div>
                    </div>
                </div>

                <div id="4.8" class="row mb-4 video">
                    <div class="col-md-4 thumbnail-wrapper">
                        <a target="_blank" href="https://www.youtube.com/watch?v=Q1mEwwC-YOg">
                            <img src="imgs/thumb4_8.png" alt="Video Thumbnail" class="img-fluid">
                        </a>
                    </div>
                    <div class="col-md-8 d-flex flex-column">
                        <h3>#4.8: Adding Attention to the Language Translation RNN in PyTorch (Lab)</h3>
                        <div class="video-description">
                            <p>In this lab we'll add a simple attention mechanism to our language translation RNN. This
                                will be similar to the method described in Bahdanau et al.'s seminal paper from 2015
                                titled "Neural Machine Translation by Jointly Learning to Align and Translate".
                            </p>
                        </div>
                        <a target="_blank" href="#" class="show-more" style="display: none;">more...</a>
                        <div class="reference-links mt-auto">
                            <a
                                href="https://colab.research.google.com/drive/1cBWMvtz0dkLJAA_X95v_6VA21Vz6B9BR?usp=sharing"><i
                                    class="fas fa-code"></i> Lab Notebook</a>
                        </div>
                    </div>
                </div>
            </div>
            <h2 id="part5">Part 5: Large Language Models and Real-World Applications</h2>
            <div class="video-section">
                <div id="5.1" class="row mb-4 video">
                    <div class="col-md-4 thumbnail-wrapper">
                        <a target="_blank" href="https://youtu.be/GhdB7UMtGqs">
                            <img src="imgs/thumb5_1.png" alt="Video Thumbnail" class="img-fluid">
                        </a>
                    </div>
                    <div class="col-md-8 d-flex flex-column">
                        <h3>#5.1: Transformers and Self-Attention</h3>
                        <div class="video-description">
                            <p>This episode covers the groundbreaking Transformer architecture introduced by Google
                                researchers in 2017. We cover the introduction of self-attention and positional encoding
                                to remove recurrence and increase parallelization, which led to greater performance and
                                scalability. </p>
                        </div>
                        <a target="_blank" href="#" class="show-more" style="display: none;">more...</a>
                        <div class="reference-links mt-auto">
                            <a href="#" data-toggle="modal" data-target="#pdfModal"
                                data-pdf-url="pdfs/llm-chronicles-5.1_transformers.pdf"><i class="fas fa-file-pdf"></i>
                                The Transformer Architecture</a>
                            <a href="#" data-toggle="modal" data-target="#pdfModal"
                                data-pdf-url="pdfs/llm-chronicles-5.1_transformers_summary.pdf"><i
                                    class="fas fa-file-pdf"></i> Transformer Summary</a>
                        </div>
                    </div>
                </div>

                <div id="5.2" class="row mb-4 video">
                    <div class="col-md-4 thumbnail-wrapper">
                        <a target="_blank" href="https://youtu.be/3LKLV6i68l0">
                            <img src="imgs/thumb5_2.png" alt="Video Thumbnail" class="img-fluid">
                        </a>
                    </div>
                    <div class="col-md-8 d-flex flex-column">
                        <h3>#5.2: Making LLMs from Transformers (BERT, Encoder-based)</h3>
                        <div class="video-description">
                            <p>This episode dives into how to build LLMs from the encoder component of Transformers.
                                Specifically, we look at the similarities and differences between the encoder and
                                decoder parts of a Transformer, we then see how Google used the encoder to build BERT
                                via pre-training and fine-tuning.</p>
                        </div>
                        <a target="_blank" href="#" class="show-more" style="display: none;">more...</a>
                        <div class="reference-links mt-auto">
                            <a href="#" data-toggle="modal" data-target="#pdfModal"
                                data-pdf-url="pdfs//llm-chronicles-5.2_encoder-based-llms-bert.pdf"><i
                                    class="fas fa-file-pdf"></i> Encoder-based LLMS, BERT</a>
                        </div>
                    </div>
                </div>

                <div id="5.3" class="row mb-4 video">
                    <div class="col-md-4 thumbnail-wrapper">
                        <a target="_blank" href="https://youtu.be/rpHpuk9sEao">
                            <img src="imgs/thumb5_2.2.png" alt="Video Thumbnail" class="img-fluid">
                        </a>
                    </div>
                    <div class="col-md-8 d-flex flex-column">
                        <h3>#5.3: Fine-tuning DistilBERT for Sentiment Analysis (Lab)</h3>
                        <div class="video-description">
                            <p>In this lab we'll see how to fine-tune DistilBERT for analyzing the sentiment of
                                restaurant reviews. We'll look at how to do this from scratch, adding the specific
                                layers for classification by hand. We'll conclude by looking at how to use the
                                HuggingFace Transformers library for this.
                            </p>
                        </div>
                        <a target="_blank" href="#" class="show-more" style="display: none;">more...</a>
                        <div class="reference-links mt-auto">
                            <a
                                href="https://colab.research.google.com/drive/1B_ERSgQDLNOL8NPCvkn7s8h_vEdZ_szI?usp=sharing"><i
                                    class="fas fa-code"></i> Lab Notebook</a>
                        </div>
                    </div>
                </div>

                <div id="5.4" class="row mb-4 video">
                    <div class="col-md-4 thumbnail-wrapper">
                        <a target="_blank" href="https://youtu.be/b3uz-V33_Ko">
                            <img src="imgs/thumb5_41.png" alt="Video Thumbnail" class="img-fluid">
                        </a>
                    </div>
                    <div class="col-md-8 d-flex flex-column">
                        <h3>#5.4: GPT, Instruction Fine-Tuning, RLHF</h3>
                        <div class="video-description">
                            <p>In this episode well see how OpenAPI built the models behind ChatGPT from the decoder
                                part of a Transformer. We'll see how Generative Pre-Trained Transformers are fined tuned
                                for instruction following in natural language and how hey are aligned using RLHF
                                (Reinforcement Learning from Human Feedback).</p>
                        </div>
                        <a target="_blank" href="#" class="show-more" style="display: none;">more...</a>
                        <div class="reference-links mt-auto">
                            <a href="#" data-toggle="modal" data-target="#pdfModal"
                                data-pdf-url="pdfs/llm-chronicles-5.4_gpt_instruction_finetuning_rlhf.pdf"><i
                                    class="fas fa-file-pdf"></i> GPT, Instruction Fine-tuning, RLHF</a>
                        </div>
                    </div>
                </div>

                <div id="5.5" class="row mb-4 video">
                    <div class="col-md-4 thumbnail-wrapper">
                        <a target="_blank" href="https://youtu.be/Tuac7zEY2iw">
                            <img src="imgs/thumb5_5.png" alt="Video Thumbnail" class="img-fluid">
                        </a>
                    </div>
                    <div class="col-md-8 d-flex flex-column">
                        <h3>#5.5: Running Gemma 2B and Llama-2 7B with Quantization</h3>
                        <div class="video-description">
                            <p>In this lab we'll get hands-on with two open-weight LLMs, Gemma 2B and Llama-2 7B. We'll
                                see how to run them inside Google Colab notebooks within the free T4 tier. We'll take a
                                look at the base and the instruction tuned versions, to understand the difference and
                                the impact of fine-tuning and RLHF on the performance of the models.</p>
                        </div>
                        <a target="_blank" href="#" class="show-more" style="display: none;">more...</a>
                        <div class="reference-links mt-auto">
                            <a
                                href="https://colab.research.google.com/drive/1DPmvkPBF6g_aj8uixMjH5Awy9fIf_Uxh?usp=sharing"><i
                                    class="fas fa-code"></i> Lab Notebook</a>
                        </div>
                    </div>
                </div>

                <div id="5.6" class="row mb-4 video">
                    <div class="col-md-4 thumbnail-wrapper">
                        <a target="_blank" href="https://youtu.be/njpV1tfIC7g">
                            <img src="imgs/thumb5_8.png" alt="Video Thumbnail" class="img-fluid">
                        </a>
                    </div>
                    <div class="col-md-8 d-flex flex-column">
                        <h3>#5.6: LLM Limitations and Challenges</h3>
                        <div class="video-description">
                            <p>In this episode we'll cover the main limitations and challenges with current Large
                                Language Models (LLM). This helps cut through the hype and get a clear picture of what
                                the technology is good for and what needs improvement.</p>
                        </div>
                        <a target="_blank" href="#" class="show-more" style="display: none;">more...</a>
                        <div class="reference-links mt-auto">
                            <a href="#" data-toggle="modal" data-target="#pdfModal"
                                data-pdf-url="pdfs/llm-chronicles-5.8_llm_limitations.pdf"><i
                                    class="fas fa-file-pdf"></i> LLM Limitations and Challenges</a>
                        </div>
                    </div>
                </div>

            </div>
            <h2 id="part6">Part 6: LLM Applications and Advancements</h2>
            <div class="video-section">
                <div id="6.1" class="row mb-4 video">
                    <div class="col-md-4 thumbnail-wrapper">
                        <a target="_blank" href="https://youtu.be/7yXVhDz3OD8">
                            <img src="imgs/thumb_6.1.png" alt="Video Thumbnail" class="img-fluid">
                        </a>
                    </div>
                    <div class="col-md-8 d-flex flex-column">
                        <h3>#6.1: Retrieval Augmented Generation - RAG (Part 1)</h3>
                        <div class="video-description">
                            <p>This episode introduces RAG (Retrieval Augmented Generation) with LLMs. We cover all the
                                basic concepts, including semantic search with vector databases. We finish with a quick
                                lab showing how to create a basic RAG system for the Huberman Lab Podcast with
                                Langchain, GPT-3.5, BGE embeddings and ChromaDB.</p>
                        </div>
                        <a target="_blank" href="#" class="show-more" style="display: none;">more...</a>
                        <div class="reference-links mt-auto">
                            <a href="#" data-toggle="modal" data-target="#pdfModal"
                                data-pdf-url="pdfs/llm-chronicles-5.3_retrieval_augmented_generation-part-1.pdf"><i
                                    class="fas fa-file-pdf"></i> Retrieval Augmented Generation - Part 1</a>
                            <a
                                href="https://colab.research.google.com/drive/1lyIskhL46RRQbpxGhi-ovnZmnPOx7Dw6?usp=sharing"><i
                                    class="fas fa-code"></i> Lab Notebook</a>
                        </div>
                    </div>
                </div>

                <div id="6.2" class="row mb-4 video">
                    <div class="col-md-4 thumbnail-wrapper">
                        <a target="_blank" href="https://www.youtube.com/watch?v=dCEMod64dko">
                            <img src="imgs/thumb_6.2.png" alt="Video Thumbnail" class="img-fluid">
                        </a>
                    </div>
                    <div class="col-md-8 d-flex flex-column">
                        <h3>#6.2: Retrieval Augmented Generation - RAG (Part 2)</h3>
                        <div class="video-description">
                            <p>This episode explores some issues with RAG, covering solutions and common strategies to
                                mitigate them and improve performance such as self-query, parent document retrieval and
                                HyDE (Hypothetical Document Embedding).</p>
                        </div>
                        <a target="_blank" href="#" class="show-more" style="display: none;">more...</a>
                        <div class="reference-links mt-auto">
                            <a href="#" data-toggle="modal" data-target="#pdfModal"
                                data-pdf-url="pdfs/llm-chronicles-5.4_retrieval_augmented_generation-full.pdf"><i
                                    class="fas fa-file-pdf"></i> Retrieval Augmented Generation - Part 1</a>
                            <a
                                href="https://colab.research.google.com/drive/10yjlRUpJGWzOWE75lY81lkKiaDXcvenA?usp=sharing"><i
                                    class="fas fa-code"></i> Notebook (Self-Query)</a>
                            <a
                                href="https://colab.research.google.com/drive/1OEauDfrYKU9CLSCQwdnGX21bjznAl5hv?usp=sharing"><i
                                    class="fas fa-code"></i> Notebook (Parent-Document)</a>
                            <a
                                href="https://colab.research.google.com/drive/1l88f2d0-krKlOmWCOdEafTX7dPPGfZrs?usp=sharing"><i
                                    class="fas fa-code"></i> Notebook (HyDE)</a>
                        </div>
                    </div>
                </div>
                <div id="6.3" class="row mb-4 video">
                    <div class="col-md-4 thumbnail-wrapper">
                        <a target="_blank" href="https://youtu.be/_sGwL6RAsUc">
                            <img src="imgs/thumb_6.3.png" alt="Video Thumbnail" class="img-fluid">
                        </a>
                    </div>
                    <div class="col-md-8 d-flex flex-column">
                        <h3>#6.3: Multi-Modal LLMs for Image, Sound and Video</h3>
                        <div class="video-description">
                            <p>In this episode we look at the architecture of a multi-modal LLM, discussing the general
                                training process of an MLLM. After that, we’ll focus on vision and explore Vision
                                Transformers and how they are trained with contrastive learning. Vision Transformers are
                                the most commonly used building block in MLLMs with vision capabilities. Finally, we’ll
                                get hands-on and look into Google’s open-weight PaliGemma, analysing its implementation
                                to see these concepts in action within a real-world multi-modal LLM.</p>
                        </div>
                        <a target="_blank" href="#" class="show-more" style="display: none;">more...</a>
                        <div class="reference-links mt-auto">
                            <a href="#" data-toggle="modal" data-target="#pdfModal"
                                data-pdf-url="pdfs/llm-chronicles-6.3_multi-modal-llm.pdf"><i
                                    class="fas fa-file-pdf"></i> Multi-Modal LLM Canvas</a>
                            <a
                                href="https://colab.research.google.com/drive/1wEkBQcYq8-xsyGlvgxpXQxjP_yN9F0FU?usp=sharing"><i
                                    class="fas fa-code"></i> PaliGemma Notebook</a>
                        </div>
                    </div>
                </div>
                <div id="6.4" class="row mb-4 video">
                    <div class="col-md-4 thumbnail-wrapper">
                        <a target="_blank" href="https://youtu.be/WKTNxaZJf4Y">
                            <img src="imgs/thumb_6.4.3.png" alt="Video Thumbnail" class="img-fluid">
                        </a>
                    </div>
                    <div class="col-md-8 d-flex flex-column">
                        <h3>#6.4: LLM Agents with ReAct (Reason + Act)</h3>
                        <div class="video-description">
                            <p>In this episode we’ll cover LLM agents, focusing on the core research (that helped to
                                improve LLMs’ reasoning while allowing them to interaction with the external world via
                                the use of tools. This includes Chain of Thought prompting, PAL (Program-aieded Language
                                Models) and ReAct (Reason + Act) as used in Langchain and CrewAI agents.
                            </p>
                        </div>
                        <a target="_blank" href="#" class="show-more" style="display: none;">more...</a>
                        <div class="reference-links mt-auto">
                            <a href="#" data-toggle="modal" data-target="#pdfModal"
                            data-pdf-url="pdfs/llm-chronicles-6.4-llm-agents_chain-of-thought_react.pdf"><i
                                class="fas fa-file-pdf"></i> LLM Agents (Chain ot Thought / ReAct) Canvas</a>
                        </div>
                    </div>
                </div>
                <div id="6.5" class="row mb-4 video">
                    <div class="col-md-4 thumbnail-wrapper">
                        <a target="_blank" href="#">
                            <img src="imgs/under_construction.png" alt="Video Thumbnail" class="img-fluid">
                        </a>
                    </div>
                    <div class="col-md-8 d-flex flex-column">
                        <h3>#6.5: Quantization and Distillation</h3>
                        <div class="video-description">
                            <p>Coming soon(ish!)</p>
                        </div>
                        <a target="_blank" href="#" class="show-more" style="display: none;">more...</a>
                        <div class="reference-links mt-auto">

                        </div>
                    </div>
                </div>
                <div id="6.6" class="row mb-4 video">
                    <div class="col-md-4 thumbnail-wrapper">
                        <a target="_blank" href="#">
                            <img src="imgs/under_construction.png" alt="Video Thumbnail" class="img-fluid">
                        </a>
                    </div>
                    <div class="col-md-8 d-flex flex-column">
                        <h3>#6.6: Transformer Architecture Improvements (MoE, Flash Attention)</h3>
                        <div class="video-description">
                            <p>Coming soon(ish!)</p>
                        </div>
                        <a target="_blank" href="#" class="show-more" style="display: none;">more...</a>
                        <div class="reference-links mt-auto">

                        </div>
                    </div>
                </div>
            </div>
        </section>
    </div>

    <!-- Modal -->

    <!-- Modal -->
    <div class="modal fade" id="pdfModal" tabindex="-1" role="dialog" aria-labelledby="pdfModalLabel"
        aria-hidden="true">
        <div class="modal-dialog modal-dialog-centered modal-fullscreen" role="document">
            <div class="modal-content">
                <div class="modal-header" style="position: relative;"> <!-- Set position to relative here -->
                    <h5 class="modal-title" id="pdfModalLabel">PDF Preview</h5>
                    <a href="#" id="downloadPdfBtn" class="btn btn-primary"
                        style="position: absolute; top: 50%; left: 50%; transform: translate(-50%, -50%);" download>
                        <!-- Absolute positioning and centering -->
                        <i class="fas fa-download"></i> Download
                    </a>
                    <button type="button" class="close" data-dismiss="modal" aria-label="Close">
                        <span aria-hidden="true">&times;</span>
                    </button>
                </div>
                <div class="modal-body">
                    <iframe id="pdfIframe" style="width: 100%; height: 80vh;" frameborder="0"></iframe>
                </div>
            </div>
        </div>
    </div>

    <!-- Sticky Footer -->
    <footer class="footer bg-dark text-white text-center py-3">
        &copy; 2024 <a target="_blank" target="_blank" href="https://www.youtube.com/@donatocapitella">Donato
            Capitella</a>
    </footer>

    <!-- Bootstrap JS and jQuery -->
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css">
    <script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.9.3/dist/umd/popper.min.js"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js"></script>
    <script src="site.js"></script>
    <script defer data-domain="llm-chronicles.com" src="https://plausible.skybound.link/js/plausible.js"></script>

</body>

</html>